# -*- coding: utf-8 -*-
"""Static Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1idgI5ALvsJOuTPTM6Gyd7t1WPa17t2-M

# **CHATBOT USING STATIC DATASET**

**Importing Useful** **Libraries**
"""

# for importing and manuplating data
import numpy as np
import pandas as pd
import string
import csv

# for fitting model
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer
from sklearn.pipeline import Pipeline

# connecting with google, request data and applying nlp
import requests
from lxml import html
# from googlesearch import search
from bs4 import BeautifulSoup
import string
import urllib.request
from urllib.request import urlopen
import re
import spacy
import os
from spacy.lang.en.stop_words import STOP_WORDS

"""**Importing Dataset From Drive**"""

# original data is in the form of google sheets
# converting sheets to csv file

def find_solution():
    sheet_url = 'https://docs.google.com/spreadsheets/d/1KqvA5O9hH62UQWYpn2DVA5Pf4Kofp4gk51hZLVSuJpw/edit#gid=0'

    csv_file = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')

    # reading dataset into variable 'df'
    df = pd.read_csv(csv_file)

    """**Working on Healthcare Dataset**"""

    # checking for empty columns

    df['Chatbot'].isnull().sum()

    # removing rows with empty columns

    df = df.dropna()

    """
    ### Dataset is now ready for use"""

    # connecting to user inputs dataset

    # from google.colab import drive
    #
    # drive.mount('/content/drive', force_remount=True)


    # input function

    def user_input():
        a = "Hello there! I am Reyana, your healthcare assistant. Please enter your details to continue."
        b = "Enter your "
        fields = ['name', 'email', 'age', 'current location']  # fields of input
        result = []

        print(a)
        for i in range(len(fields)):
            print(b + fields[i])
            inp = input()
            result.append(inp)

        # input to a csv file
        filename = os.path.join(os.path.dirname(os.path.dirname(__file__)), "home/chatbot_static/User_dataset (1).csv")

        with open(filename, 'a+', newline='\n', encoding='UTF8') as f:
            writer = csv.writer(f)
            writer.writerow(result)
            f.close()


    # calling input function

    # user_input()

    # display csv file head

    inputs = pd.read_csv(os.path.join(os.path.dirname(os.path.dirname(__file__)),"home/chatbot_static/User_dataset (1).csv"))

    inputs.head()

    """***Decision Tree Classifier***"""


    # function to remove punctuation
    # converts the input to lowercase

    def cleaner(x):
        return [a for a in (''.join([a for a in x if a not in string.punctuation])).lower().split()]


    # Creating a pipeline to facilitate conversation
    # Using Decision Tree for fitting data

    Pipe = Pipeline([
        ('bow', CountVectorizer(analyzer=cleaner)),
        ('tfidf', TfidfTransformer()),
        ('classifier', DecisionTreeClassifier())
    ])

    # Fitting our dataset in the pipeline

    Pipe.fit(df['User'], df['Chatbot'])

    """***Talking to chatbot***"""

    input = ""
    input = 'MALARIA'

    output = Pipe.predict([input])[0]

    """Collecting Data from Google"""

    # uploading small English pipeline trained on written web text from Spacy

    nlp = spacy.load("en_core_web_sm")

    # query name
    query = 'diabetes prevention'

    # getting top 10 links from google and storing it in a list
    search_result_list = list(search(query, tld="co.in", num=2, stop=1, pause=1))

    print(*search_result_list, sep='\n')

    """Extracting Results"""

    # extracting html format of website

    url = search_result_list[0]

    r = requests.get(url)
    html = r.text

    # converting to text

    soup = BeautifulSoup(html, "html.parser")

    text = soup.get_text()

    # working on text data

    for script in soup(["script", "style"]):
        script.extract()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    # print(text)

    # printing results

    resultant = soup.findAll('h3')[:10]
    print(*resultant, sep='\n')

    # removing tags, printing final result

    sol = []

    for i in range(len(resultant)):
        sol.append(resultant[i].text)
    return sol
    # print(*sol, sep='\n')


if __name__ == "__main__":
    find_solution()
